{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47d6829-bf71-4899-8d5b-a216f13c4807",
   "metadata": {},
   "source": [
    "# Results Visualizer\n",
    "This notebook contains a set of visualizations and graphs of the processed results, based on the different hypotheses and conducted experiments. The format is as follows:\n",
    "- Import of required packages and initialization of parameters that are used throughout the notebook\n",
    "- A section is dedicated for each of the hypotheses of the thesis that contains more information about the hypothesis, the experiments and the respective results, along with visualizations and graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ff4be-6cd0-4017-a00b-3ce2cd8d4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "# matplotlib params\n",
    "plt.rcParams.update({\n",
    "    'font.size': 18,\n",
    "    'lines.linewidth': 3,\n",
    "    'axes.titlesize': 18,\n",
    "    'figure.titlesize': 'x-large'\n",
    "})\n",
    "# save figures flag\n",
    "save_figs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b80f7b-f7d8-46cb-aec2-e9ebc4dfe2eb",
   "metadata": {},
   "source": [
    "## PART 0 - Metadata\n",
    "In the output directory of each experiment, there is `metadata.csv` file that stores the configuration of the individual runs for each experiment:\n",
    "- **dataset_name**: the name of the dataset\n",
    "- **dataset_iter**: specific train/test split of the dataset used\n",
    "- **search_type**: `'random', 'ue' or 'smac'`\n",
    "- **num_classifiers**: list of PyOD classifiers used\n",
    "- **validation_strategy**: `stratified` or `balanced`\n",
    "- **validation_size**: `20`, `50`, `100`, `200` or `400`\n",
    "- **total_budget**: total budget in seconds\n",
    "- **per_run_budget**: budget per individual run in seconds\n",
    "\n",
    "From these values, the aggregated experiment parameters are calculated and passed later on to the plotting function for each hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e847171-7d26-4b8b-bbf0-3150470175c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directories\n",
    "results_dirname = '../results/results'\n",
    "results_path = os.path.join(Path.cwd(), results_dirname)\n",
    "results_processed_path = os.path.join(results_path, 'processed')\n",
    "# Import metadata\n",
    "metadata_filepath = os.path.join(results_path, 'metadata.csv')\n",
    "metadata_df = pd.read_csv(metadata_filepath)\n",
    "# Remove individual ue (random search) runs\n",
    "metadata_df = metadata_df[metadata_df['total_budget'] != 30]\n",
    "# Extract experiment parameters\n",
    "total_budget = metadata_df.total_budget[0]\n",
    "dataset_list = list(metadata_df.dataset_name.unique())\n",
    "validation_strategy_list = list(metadata_df.validation_strategy.unique())\n",
    "validation_size_list = list(metadata_df.validation_size.unique())\n",
    "# Default width and height for plots\n",
    "if len(dataset_list) > 3:\n",
    "    w0 = 4\n",
    "    h0 = 4\n",
    "else:\n",
    "    w0 = 5\n",
    "    h0=5\n",
    "# Print the parameters\n",
    "print('Total budget:', total_budget)\n",
    "print('Dataset list:', dataset_list)\n",
    "print('Validation strategy list:', validation_strategy_list)\n",
    "print('Validation size list:', validation_size_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74f51e-c2de-4c9a-877b-370450a08c33",
   "metadata": {},
   "source": [
    "## PART 1 - Performance Summary\n",
    "### 1.1 - Performance Summary per Dataset\n",
    "This section parses the processed results for each dataset and prints the best optimization and test scores along with the parameters that achieved them:\n",
    " - Search algorithm\n",
    " - Validation set split strategy\n",
    " - Validation set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafefaa5-5c4c-4f99-ae28-2b8f83f17eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance per dataset\n",
    "for dataset in dataset_list:\n",
    "    best_opt_score = 0.0\n",
    "    best_test_score = 0.0\n",
    "    best_opt_params = None\n",
    "    best_test_params = None\n",
    "    for filename in os.listdir(results_processed_path):\n",
    "        if dataset in filename:\n",
    "            # import performance data as DataFrame\n",
    "            df = pd.read_csv(\n",
    "                os.path.join(\n",
    "                    results_processed_path, filename),\n",
    "            )\n",
    "            # optimization score\n",
    "            if df['single_best_optimization_score'].iloc[-1] > best_opt_score:\n",
    "                best_opt_score = df['single_best_optimization_score'].iloc[-1]\n",
    "                best_opt_params = filename.split('.')[0].split('_')[1:]\n",
    "            # test score\n",
    "            if df['single_best_test_score'].iloc[-1] > best_test_score:\n",
    "                best_test_score = df['single_best_test_score'].iloc[-1]\n",
    "                best_test_params = filename.split('.')[0].split('_')[1:]\n",
    "    print(dataset)\n",
    "    print('  Opt\\t ({}, {})'.format(best_opt_score, best_opt_params))\n",
    "    print('  Test\\t ({}, {})\\n'.format(best_test_score, best_test_params))\n",
    "    print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d04ef3-f711-4571-932b-0ce04902688f",
   "metadata": {},
   "source": [
    "### 1.2 - Performance Summary per Dataset-Search Algorithm\n",
    "This section parses the processed results for each dataset and and search algorithm and prints the best optimization and test scores along with the parameters that achieved them:\n",
    " - Validation set split strategy\n",
    " - Validation set size\n",
    " \n",
    "Furthermore, it computes statistical tests for comparing the 3 search algorithms:\n",
    " - Friedmann test\n",
    " - Nemenyi post-hoc test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a14ce1-8e12-4bcc-821c-628a9a1efc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance per (dataset, search algorithm) tuples\n",
    "search_algorithm_list = ['random', 'ue', 'smac']\n",
    "np_matrix = np.zeros((len(dataset_list), 3)) # number of search algorithms = 3\n",
    "for i, dataset in enumerate(dataset_list):\n",
    "    print(dataset)\n",
    "    for j, search in enumerate(search_algorithm_list):\n",
    "        best_opt_score = 0.0\n",
    "        best_test_score = 0.0\n",
    "        best_opt_params = None\n",
    "        best_test_params = None\n",
    "        for filename in os.listdir(results_processed_path):\n",
    "            if dataset in filename and search in filename:\n",
    "                # import performance data as DataFrame\n",
    "                df = pd.read_csv(\n",
    "                    os.path.join(\n",
    "                        results_processed_path, filename),\n",
    "                )\n",
    "                # optimization score\n",
    "                if df['single_best_optimization_score'].iloc[-1] > best_opt_score:\n",
    "                    best_opt_score = df['single_best_optimization_score'].iloc[-1]\n",
    "                    best_opt_params = filename.split('.')[0].split('_')[2:]\n",
    "                # test score\n",
    "                if df['single_best_test_score'].iloc[-1] > best_test_score:\n",
    "                    best_test_score = df['single_best_test_score'].iloc[-1]\n",
    "                    best_test_params = filename.split('.')[0].split('_')[2:]\n",
    "        np_matrix[i][j] = best_test_score\n",
    "        print('  ' + search)\n",
    "        print('\\tOpt\\t ({}, {})'.format(best_opt_score, best_opt_params))\n",
    "        print('\\tTest\\t ({}, {})\\n'.format(best_test_score, best_test_params))\n",
    "    print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')\n",
    "# construct pd matrix\n",
    "matrix = pd.DataFrame(\n",
    "    np_matrix, \n",
    "    columns=search_algorithm_list, \n",
    "    index=dataset_list\n",
    ")\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71c74c-41d9-432b-b2b1-eb5a965b4b4a",
   "metadata": {},
   "source": [
    "### 1.3 - Statistical Comparison of Search Algorithms\n",
    "This section conducts a non-parametric Friedman test to assess the significance of difference in average performance of N different search algorithms on M datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40697bf9-820e-4cb5-8bfa-ba50cf4c16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison of search algorithms\n",
    "# Add ML-stats directory to sys\n",
    "import os, sys\n",
    "p = os.path.abspath('../ML-stats')\n",
    "sys.path.insert(1, p)\n",
    "# Imports\n",
    "from src.classifier_comparisons import BlockDesign\n",
    "from src.multiple_classifiers import friedman_test\n",
    "from src.multiple_classifiers import nemenyi_friedman_test\n",
    "# Block design\n",
    "block_design = BlockDesign(matrix, threshold=0.01, precision=2, higher_is_better=True)\n",
    "# Friedman test\n",
    "test_results = friedman_test(block_design, alpha=0.05)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06793bf2-8e26-4bf3-9227-b5063b9b8e03",
   "metadata": {},
   "source": [
    "## PART 2 - Evaluation of Hypotheses\n",
    "### 2.1 - H1: Guided vs. unguided search\n",
    "\n",
    "**Statement**: *Guided search algorithms such as SMAC are expected to perform better than unguided search algorithms such as Random Search and Uniform Exploration.*\n",
    "\n",
    "**Comments**:\n",
    "- The comparison should be done per dataset.\n",
    "- The comparison should be done for a specified validation strategy (e.g. stratified) and size (e.g. a percentage 30% of the original training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e92e3-3f5e-4dc3-b613-3916888e3563",
   "metadata": {},
   "source": [
    "### Generic plotting functions\n",
    "The plotting functions used in the hypotheses sections. were made generic so the generated plots can depend only on the input arguments:\n",
    "- **output_performance_path**: path to the `performance` directory\n",
    "- **dataset_list**: list of datasets used in the experiment\n",
    "- **total_budget**: total budget in seconds\n",
    "- **validation_strategy_list**: `['stratified', 'balanced']`\n",
    "- **validation_size_list**: `[20, 50, 100, 200, 400]`\n",
    "- **eval_type**: `opt` or `test`\n",
    "- **plot_color_list**: used for consistent coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491d042-a9fa-4a06-9193-546b08b62762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function for H1\n",
    "def plot_h1_results(\n",
    "    results_processed_path, # str\n",
    "    dataset_list, # list(str)\n",
    "    total_budget, # int\n",
    "    validation_strategy, # int\n",
    "    validation_size, # int\n",
    "    eval_type, # str\n",
    "    color_list # list(str)\n",
    "):\n",
    "    # Figure\n",
    "    width = w0 * len(dataset_list)\n",
    "    height = h0 * len(dataset_list)\n",
    "    fig = plt.figure(\n",
    "        figsize=(width, height)) # grid dimensions\n",
    "    fig.subplots_adjust(\n",
    "        wspace=0.4, hspace=0.3) # space between plots\n",
    "    fig.suptitle(\n",
    "        'Performance on {} set for {} validation set of size {}'\\\n",
    "        .format(\n",
    "            eval_type,\n",
    "            validation_strategy,\n",
    "            validation_size),\n",
    "        y=0.93\n",
    "    )\n",
    "    # Plots\n",
    "    for i, dataset in enumerate(dataset_list):\n",
    "        for filename in os.listdir(results_processed_path):\n",
    "            if dataset in filename \\\n",
    "            and validation_strategy in filename \\\n",
    "            and str(validation_size)+'.' in filename:\n",
    "                # import performance data as DataFrame\n",
    "                df = pd.read_csv(\n",
    "                    os.path.join(\n",
    "                        results_processed_path, filename),\n",
    "                )\n",
    "                # x-axis (time)\n",
    "                x = df.Timestamp\n",
    "                # score based on eval_type\n",
    "                if eval_type == 'opt':\n",
    "                    y = df.single_best_optimization_score\n",
    "                elif eval_type == 'test':\n",
    "                    y = df.single_best_test_score\n",
    "                # plot\n",
    "                label = filename.split('_')[1]\n",
    "                ax = plt.subplot(3, 3, i + 1)\n",
    "                ax.set_ylim([0.5, 1.])\n",
    "                ax.set_xlabel('seconds')\n",
    "                ax.set_ylabel('score')\n",
    "                ax.plot(x, y, label=label, color=color_list[label])\n",
    "                ax.grid()\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                labels, handles = zip(\n",
    "                    *sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "                ax.legend(handles, labels, loc='lower right')\n",
    "                plt.title(dataset)\n",
    "                if save_figs:\n",
    "                    plt.savefig('{}_{}_{}_{}.png'.format(\n",
    "                        eval_type,\n",
    "                        total_budget,\n",
    "                        validation_strategy,\n",
    "                        validation_size\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5a1b2-a8de-43d6-bf11-1fe0a14fd0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H1\n",
    "validation_size_h1 = 200\n",
    "color_list_h1 = {\n",
    "    'ue': 'orange',\n",
    "    'random': 'green',\n",
    "    'smac': 'royalblue',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53d0b4-fd35-46db-94c8-302300c8a357",
   "metadata": {},
   "source": [
    "#### 2.1.1 - Performance for stratified validation sets\n",
    "The below graphs display the optimization performance, i.e. the performance on the validation set used in Bayesian Optimization, per dataset, for the case of `stratified` validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f836807-e82e-4b32-9727-e1791558c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H1.A\n",
    "validation_strategy_h11 = 'stratified'\n",
    "# Plot optimization performance\n",
    "plot_h1_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    total_budget,\n",
    "    validation_strategy_h11,\n",
    "    validation_size_h1,\n",
    "    'opt',\n",
    "    color_list_h1\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_h1_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    total_budget,\n",
    "    validation_strategy_h11,\n",
    "    validation_size_h1,\n",
    "    'test',\n",
    "    color_list_h1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83426d1b-632e-4eef-8d7d-c2cd2d6d84fb",
   "metadata": {},
   "source": [
    "#### 2.1.2 - Performance for balanced validation sets\n",
    "The below graphs display the test set performance per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17dbb1-4a49-4bee-b2b3-e2c213ec699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H1.B\n",
    "validation_strategy_h12 = 'balanced'\n",
    "# Plot optimization performance\n",
    "plot_h1_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    total_budget,\n",
    "    validation_strategy_h12,\n",
    "    validation_size_h1,\n",
    "    'opt',\n",
    "    color_list_h1\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_h1_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    total_budget,\n",
    "    validation_strategy_h12,\n",
    "    validation_size_h1,\n",
    "    'test',\n",
    "    color_list_h1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a4ab90-dae6-4ed0-addc-3218c859468d",
   "metadata": {},
   "source": [
    "### 2.2 - H2: Stratified vs. balanced validation sets\n",
    "\n",
    "**Statement**: *Stratified validation sets are expected to have better performance compared to balanced validation sets.*\n",
    "\n",
    "**Comments**:\n",
    "- The expectation of better performance when using stratified validation sets stems from the fact that stratified-sampled validation sets better represent the actual data distribution.\n",
    "- The comparison should be done per dataset.\n",
    "- The comparison should be done per search algorithm (uniform exploration, random search, smac)\n",
    "- The comparison should be done for a specified validation set size (e.g. 30% of the original training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe09c46-1196-475b-8c77-787852ceed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function for H2\n",
    "def plot_h2_results(\n",
    "    results_processed_path, # str\n",
    "    dataset_list, # list(str)\n",
    "    search_algorithm, # str\n",
    "    total_budget, # int\n",
    "    validation_size, # int\n",
    "    eval_type, # str\n",
    "    color_list # list(str)\n",
    "):\n",
    "    # Figure\n",
    "    width = w0 * len(dataset_list)\n",
    "    height = h0 * len(dataset_list)\n",
    "    fig = plt.figure(\n",
    "        figsize=(width, height)) # grid dimensions\n",
    "    fig.subplots_adjust(\n",
    "        wspace=0.4, hspace=0.3) # space between plots\n",
    "    fig.suptitle(\n",
    "        'Performance on {} set for {} search and validation set of size {}'\\\n",
    "        .format(\n",
    "            eval_type,\n",
    "            search_algorithm,\n",
    "            validation_size),\n",
    "        y=0.93\n",
    "    )\n",
    "    # Plots\n",
    "    for i, dataset in enumerate(dataset_list):\n",
    "        for filename in os.listdir(results_processed_path):\n",
    "            if dataset in filename and \\\n",
    "            search_algorithm in filename and \\\n",
    "            str(validation_size)+'.' in filename:\n",
    "                # import performance data as DataFrame\n",
    "                df = pd.read_csv(\n",
    "                    os.path.join(\n",
    "                        results_processed_path, filename)\n",
    "                )\n",
    "                # x-axis (time)\n",
    "                x = df.Timestamp\n",
    "                # score based on eval_type\n",
    "                if eval_type == 'opt':\n",
    "                    y = df.single_best_optimization_score\n",
    "                elif eval_type == 'test':\n",
    "                    y = df.single_best_test_score\n",
    "                # plot\n",
    "                label = filename.split('_')[2]\n",
    "                ax = plt.subplot(3, 3, i + 1)\n",
    "                ax.set_ylim([0.5, 1.])\n",
    "                ax.set_xlabel('seconds')\n",
    "                ax.set_ylabel('score')\n",
    "                ax.plot(x, y, label=label, color=color_list[label])\n",
    "                ax.grid()\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                labels, handles = zip(\n",
    "                    *sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "                ax.legend(handles, labels, loc='lower right')\n",
    "                plt.title(dataset)\n",
    "                if save_figs:\n",
    "                    plt.savefig('{}_{}_{}_{}.png'.format(\n",
    "                        eval_type,\n",
    "                        total_budget,\n",
    "                        search_algorithm,\n",
    "                        validation_size\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a8fbe-1847-470e-961e-68a9c9b7825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H2\n",
    "validation_size_h2 = 100\n",
    "color_list_h2 = {\n",
    "    'stratified': 'orange',\n",
    "    'balanced': 'green',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf68b9-e121-4777-a2ee-efb27e4bd6a9",
   "metadata": {},
   "source": [
    "#### 2.2.1 - SMAC Search\n",
    "The below graphs display the test set performance per dataset for the SMAC search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1502c-ee84-4d4f-baac-a21dd5543f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H2.1\n",
    "search_algorithm_h21 = 'smac'\n",
    "# Plot optimization performance\n",
    "plot_h2_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h21,\n",
    "    total_budget,\n",
    "    validation_size_h2,\n",
    "    'opt',\n",
    "    color_list_h2\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_h2_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h21,\n",
    "    total_budget,\n",
    "    validation_size_h2,\n",
    "    'test',\n",
    "    color_list_h2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4daddd-b373-4951-b7d6-8500e9645a4e",
   "metadata": {},
   "source": [
    "#### 2.2.2 - Random Search\n",
    "The below graphs display the test set performance per dataset for *Random Search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d39f4-58ae-406c-8426-0218b887a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H2.2\n",
    "search_algorithm_h22 = 'random'\n",
    "# Plot optimization performance\n",
    "plot_h2_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h22,\n",
    "    total_budget,\n",
    "    validation_size_h2,\n",
    "    'opt',\n",
    "    color_list_h2\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_h2_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h22,\n",
    "    total_budget,\n",
    "    validation_size_h2,\n",
    "    'test',\n",
    "    color_list_h2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e1d42-9bfb-4d48-946c-803d975242fa",
   "metadata": {},
   "source": [
    "#### 2.2.3 - Uniform Exploration Search\n",
    "The below graphs display the test set performance per dataset for the *Uniform Exploration Search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bdda9-0eb7-4d16-a61d-d362fe95879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H2.3\n",
    "search_algorithm_h23 = 'ue'\n",
    "# Plot optimization performance\n",
    "plot_h2_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h23,\n",
    "    total_budget,\n",
    "    validation_size_h2,\n",
    "    'opt',\n",
    "    color_list_h2\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_h2_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h23,\n",
    "    total_budget,\n",
    "    validation_size_h2,\n",
    "    'test',\n",
    "    color_list_h2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021a1ac-1147-4721-9c2a-3ab90841b016",
   "metadata": {},
   "source": [
    "### 2.3 - H3: Larger vs. smaller validation sets\n",
    "\n",
    "**Statement**: *Larger validation sets are expected to have better performance compared to smaller validation sets.*\n",
    "\n",
    "**Comments**:\n",
    "- Validation set sizes to compare: 20, 50, 100, 200\n",
    "- Comparison should be done per dataset\n",
    "- Comparison should be done per search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a296ae2-5bbd-43a2-a049-ab60a7e930b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function for H3\n",
    "def plot_h3_results(\n",
    "    results_processed_path, # str\n",
    "    dataset_list, # list(str)\n",
    "    search_algorithm, # str\n",
    "    total_budget, # int\n",
    "    validation_strategy, # str\n",
    "    validation_size_list, # list(int)\n",
    "    eval_type, # str\n",
    "    color_list # list(str) \n",
    "):\n",
    "    # Figure\n",
    "    width = w0 * len(dataset_list)\n",
    "    height = h0 * len(dataset_list)\n",
    "    fig = plt.figure(\n",
    "        figsize=(width, height)) # grid dimensions\n",
    "    fig.subplots_adjust(\n",
    "        wspace=0.4, hspace=0.3) # space between plots\n",
    "    fig.suptitle(\n",
    "        'Performance on {} set for {} search and {} validation set'\\\n",
    "        .format(\n",
    "            eval_type,\n",
    "            search_algorithm,\n",
    "            validation_strategy),\n",
    "        y=0.93\n",
    "    )\n",
    "    # Plots\n",
    "    for i, dataset in enumerate(dataset_list):\n",
    "        for filename in os.listdir(results_processed_path):\n",
    "            if (dataset in filename and \\\n",
    "                search_algorithm in filename and \\\n",
    "                validation_strategy in filename \\\n",
    "                and any(str(size) in filename for size in validation_size_list)):\n",
    "                # import performance data as DataFrame\n",
    "                df = pd.read_csv(\n",
    "                    os.path.join(\n",
    "                        results_processed_path,\n",
    "                        filename\n",
    "                    )\n",
    "                )\n",
    "                # x-axis (seconds)\n",
    "                x = df.Timestamp\n",
    "                # score based on eval_type\n",
    "                if eval_type == 'opt':\n",
    "                    y = df.single_best_optimization_score\n",
    "                elif eval_type == 'test':\n",
    "                    y = df.single_best_test_score\n",
    "                # plot\n",
    "                label = int(filename.split('_')[3].split('.')[0])\n",
    "                ax = plt.subplot(3, 3, i + 1)\n",
    "                ax.set_ylim([0.2, 1.])\n",
    "                ax.set_xlabel('seconds')\n",
    "                ax.set_ylabel('score')\n",
    "                ax.plot(x, y, label=label, color=color_list[label])\n",
    "                ax.grid()\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                labels = [int(l) for l in labels] # convert to integer for sorting\n",
    "                labels, handles = zip(\n",
    "                *sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "                ax.legend(handles, labels, loc='lower right')\n",
    "                plt.title(dataset)\n",
    "                if save_figs:\n",
    "                    plt.savefig('{}_{}_{}_{}.png'.format(\n",
    "                        eval_type,\n",
    "                        total_budget,\n",
    "                        search_algorithm,\n",
    "                        validation_strategy\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41368773-d1c2-41f6-b35e-326f848d5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H3\n",
    "validation_strategy_h3 = 'stratified'\n",
    "color_list_h3 = {\n",
    "    20: 'red',\n",
    "    50: 'orange',\n",
    "    100: 'green',\n",
    "    200: 'royalblue',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9d337-5feb-4750-b786-1dee571ed4cb",
   "metadata": {},
   "source": [
    "#### 2.3.1 - SMAC Search\n",
    "The below graphs display the test set performance per dataset for the **SMAC Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e4c54-5a00-43c6-902d-b5d2b28bd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H3.1\n",
    "search_algorithm_h31 = 'smac'\n",
    "# Plot optimization performance\n",
    "plot_h3_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h31,\n",
    "    total_budget,\n",
    "    validation_strategy_h3,\n",
    "    validation_size_list,\n",
    "    'opt',\n",
    "    color_list_h3\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_h3_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h31,\n",
    "    total_budget,\n",
    "    validation_strategy_h3,\n",
    "    validation_size_list,\n",
    "    'test',\n",
    "    color_list_h3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d1732c-570c-4eba-9a82-50307dff7b5f",
   "metadata": {},
   "source": [
    "#### 2.3.2 - Random Search\n",
    "The below graphs display the test set performance per dataset for **Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b83907-8513-4608-80d3-d5afd1fac1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H3.2\n",
    "search_algorithm_h32 = 'random'\n",
    "# Plot optimization performance\n",
    "plot_h3_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h32,\n",
    "    total_budget,\n",
    "    validation_strategy_h3,\n",
    "    validation_size_list,\n",
    "    'opt',\n",
    "    color_list_h3\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_h3_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h32,\n",
    "    total_budget,\n",
    "    validation_strategy_h3,\n",
    "    validation_size_list,\n",
    "    'test',\n",
    "    color_list_h3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18247e-10ec-4c81-93db-251130cce6a6",
   "metadata": {},
   "source": [
    "#### 2.3.3 - Uniform Exploration Search\n",
    "The below graphs display the test set performance per dataset for the **Uniform Exploration Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7746e3e-f40f-4045-a3b6-94e77277beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for H3.3\n",
    "search_algorithm_h33 = 'ue'\n",
    "# Plot optimization performance\n",
    "plot_h3_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h33,\n",
    "    total_budget,\n",
    "    validation_strategy_h3,\n",
    "    validation_size_list,\n",
    "    'opt',\n",
    "    color_list_h3\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_h3_results(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    search_algorithm_h33,\n",
    "    total_budget,\n",
    "    validation_strategy_h3,\n",
    "    validation_size_list,\n",
    "    'test',\n",
    "    color_list_h3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17604ec-61f3-480d-b091-7c300ebabdc7",
   "metadata": {},
   "source": [
    "## Part 3 - Individual plots with confidence intervals\n",
    "### 3.1 - Compare performance across datasets for a specified search algorithm\n",
    "This section provides performance plots with standard deviation included, for single runs with well-defined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b69bbc-2d3e-438b-afbf-cfa839ce62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function with std\n",
    "def plot_results_with_std_1(\n",
    "    results_processed_path, # str\n",
    "    dataset_list, # list(str)\n",
    "    total_budget, # int\n",
    "    search_algorithm, # str\n",
    "    validation_strategy, # int\n",
    "    validation_size, # int\n",
    "    eval_type # str\n",
    "):\n",
    "    # Figure\n",
    "    width = w0 * len(dataset_list)\n",
    "    height = h0 * len(dataset_list)\n",
    "    fig = plt.figure(\n",
    "        figsize=(width, height)) # grid dimensions\n",
    "    fig.subplots_adjust(\n",
    "        wspace=0.4, hspace=0.3) # space between plots\n",
    "    fig.suptitle(\n",
    "        '{} performance on {} set for {} validation set of size {}'\\\n",
    "        .format(\n",
    "            search_algorithm,\n",
    "            eval_type,\n",
    "            validation_strategy,\n",
    "            validation_size),\n",
    "        y=0.93\n",
    "    )\n",
    "    # Plots\n",
    "    for i, dataset in enumerate(dataset_list):\n",
    "        for filename in os.listdir(results_processed_path):\n",
    "            if dataset in filename \\\n",
    "            and search_algorithm in filename \\\n",
    "            and validation_strategy in filename \\\n",
    "            and str(validation_size)+'.' in filename:\n",
    "                # import performance data as DataFrame\n",
    "                df = pd.read_csv(\n",
    "                    os.path.join(\n",
    "                        results_processed_path, filename),\n",
    "                )\n",
    "                # x-axis (time)\n",
    "                x = df.Timestamp\n",
    "                # score based on eval_type\n",
    "                if eval_type == 'opt':\n",
    "                    y = df.single_best_optimization_score\n",
    "                    dy = df.single_best_optimization_score_std\n",
    "                elif eval_type == 'test':\n",
    "                    y = df.single_best_test_score\n",
    "                    dy = df.single_best_test_score_std\n",
    "                # plot\n",
    "                label = filename.split('_')[1]\n",
    "                ax = plt.subplot(3, 3, i + 1)\n",
    "                ax.set_ylim([0.5, 1.])\n",
    "                ax.set_xlabel('seconds')\n",
    "                ax.set_ylabel('score')\n",
    "                ax.plot(x, y, label=label)\n",
    "                ax.fill_between(x, y - dy, y + dy, alpha=0.2)\n",
    "                ax.grid()\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                labels, handles = zip(\n",
    "                    *sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "                ax.legend(handles, labels, loc='lower right')\n",
    "                plt.title(dataset)\n",
    "                if save_figs:\n",
    "                    plt.savefig('{}_{}_{}_{}_std.png'.format(\n",
    "                        search_algorithm,\n",
    "                        eval_type,\n",
    "                        total_budget,\n",
    "                        validation_strategy,\n",
    "                        validation_size\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd7ea0-88da-4e1c-8dab-dcb77ca1017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "search_algorithm_31 = 'ue'\n",
    "validation_strategy_31 = 'balanced'\n",
    "validation_size_31 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5203d9-baac-46c0-acc0-1322b9ead7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation set performance\n",
    "plot_results_with_std_1(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    total_budget,\n",
    "    search_algorithm_31,\n",
    "    validation_strategy_31,\n",
    "    validation_size_31,\n",
    "    'opt'\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_results_with_std_1(\n",
    "    results_processed_path,\n",
    "    dataset_list,\n",
    "    total_budget,\n",
    "    search_algorithm_31,\n",
    "    validation_strategy_31,\n",
    "    validation_size_31,\n",
    "    'test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf7512-c9ab-4213-9946-8d8c1df2c3dd",
   "metadata": {},
   "source": [
    "### 3.2 - Compare performance across search algorithms for a specified dataset\n",
    "This section compares the scores achieved by the search algorithms on the same dataset provided as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88d885-52af-4d34-afe3-c25edd759275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function with std\n",
    "def plot_results_with_std_2(\n",
    "    results_processed_path, # str\n",
    "    dataset, # list(str)\n",
    "    total_budget, # int\n",
    "    search_algorithm_list, # str\n",
    "    validation_strategy, # int\n",
    "    validation_size, # int\n",
    "    eval_type # str\n",
    "):\n",
    "    # Figure\n",
    "    width = w0 * len(dataset_list)\n",
    "    height = h0 * len(dataset_list)\n",
    "    fig = plt.figure(\n",
    "        figsize=(width, height)) # grid dimensions\n",
    "    fig.subplots_adjust(\n",
    "        wspace=0.4, hspace=0.3) # space between plots\n",
    "    fig.suptitle(\n",
    "        'Performance on {} set for {} validation set of size {}'\\\n",
    "        .format(\n",
    "            eval_type,\n",
    "            validation_strategy,\n",
    "            validation_size),\n",
    "        y=0.93\n",
    "    )\n",
    "    # Plots\n",
    "    for i, search_algorithm in enumerate(search_algorithm_list):\n",
    "        for filename in os.listdir(results_processed_path):\n",
    "            if search_algorithm in filename \\\n",
    "            and dataset in filename \\\n",
    "            and validation_strategy in filename \\\n",
    "            and str(validation_size)+'.' in filename:\n",
    "                # import performance data as DataFrame\n",
    "                df = pd.read_csv(\n",
    "                    os.path.join(\n",
    "                        results_processed_path, filename),\n",
    "                )\n",
    "                # x-axis (time)\n",
    "                x = df.Timestamp\n",
    "                # score based on eval_type\n",
    "                if eval_type == 'opt':\n",
    "                    y = df.single_best_optimization_score\n",
    "                    dy = df.single_best_optimization_score_std\n",
    "                elif eval_type == 'test':\n",
    "                    y = df.single_best_test_score\n",
    "                    dy = df.single_best_test_score_std\n",
    "                # plot\n",
    "                label = filename.split('_')[1]\n",
    "                ax = plt.subplot(3, 3, i + 1)\n",
    "                ax.set_ylim([0.5, 1.])\n",
    "                ax.set_xlabel('seconds')\n",
    "                ax.set_ylabel('score')\n",
    "                ax.plot(x, y, label=label)\n",
    "                ax.fill_between(x, y - dy, y + dy, alpha=0.2)\n",
    "                ax.grid()\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                labels, handles = zip(\n",
    "                    *sorted(zip(labels, handles), key=lambda t: t[0]))\n",
    "                ax.legend(handles, labels, loc='lower right')\n",
    "                plt.title(dataset)\n",
    "                if save_figs:\n",
    "                    plt.savefig('{}_{}_{}_{}_std.png'.format(\n",
    "                        dataset,\n",
    "                        eval_type,\n",
    "                        total_budget,\n",
    "                        validation_strategy,\n",
    "                        validation_size\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83836d43-035e-4d34-90c9-74667800e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for 3.2\n",
    "dataset_32 = 'Cardiotocography'\n",
    "validation_strategy_32 = 'balanced'\n",
    "validation_size_32 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fdd76-e0ef-4dc5-b0c5-d5b18b96f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation set performance\n",
    "plot_results_with_std_2(\n",
    "    results_processed_path,\n",
    "    dataset_32,\n",
    "    total_budget,\n",
    "    search_algorithm_list,\n",
    "    validation_strategy_32,\n",
    "    validation_size_32,\n",
    "    'opt'\n",
    ")\n",
    "# Plot test set performance\n",
    "plot_results_with_std_2(\n",
    "    results_processed_path,\n",
    "    dataset_32,\n",
    "    total_budget,\n",
    "    search_algorithm_list,\n",
    "    validation_strategy_32,\n",
    "    validation_size_32,\n",
    "    'test'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
