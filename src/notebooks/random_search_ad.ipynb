{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASH for anomaly detection using random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, sys, time, warnings, random\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, \\\n",
    "    StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "# Hide warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Add parent path to sys\n",
    "p = os.path.abspath('..')\n",
    "sys.path.insert(1, p)\n",
    "# Import from parent\n",
    "from utils import import_dataset, create_search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of outliers in training set: 0.02\n",
      "Ratio of outliers in test set: 0.02\n",
      "Training size: 3750\n",
      "Test size: 1250\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "dataset = '../../data/Annthyroid_withoutdupl_norm_02_v01.arff'\n",
    "N = 5000 # number of max data points\n",
    "df = import_dataset(dataset)\n",
    "if(len(df) > N):\n",
    "    df = df.sample(n=N)\n",
    "# Extract X, y\n",
    "X  = df.iloc[:, :-1]\n",
    "y = df['outlier']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=10)\n",
    "# Stats\n",
    "print(\"Ratio of outliers in training set:\", len(y_train[y_train==1])/len(y_train))\n",
    "print(\"Ratio of outliers in test set:\", len(y_test[y_test==1])/len(y_test))\n",
    "print(\"Training size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of algorithms: 11\n"
     ]
    }
   ],
   "source": [
    "# Create algorithm entries and their search spaces\n",
    "models, search_space, evaluated = create_search_space()\n",
    "print('Number of algorithms:', len(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cblof:\t 12\n",
      "copod:\t 16\n",
      "ecod:\t 9\n",
      "hbos:\t 12\n",
      "ifor:\t 9\n",
      "knn:\t 14\n",
      "lof:\t 11\n",
      "mcd:\t 14\n",
      "ocsvm:\t 15\n",
      "pca:\t 16\n",
      "sos:\t 12\n",
      "Best model: mcd\n",
      "Hyperparameter configuration: {'assume_centered': True, 'contamination': 0.06951995318634668, 'support_fraction': 0.7340868676428925}\n",
      "ROC AUC score: 0.9422140770860775\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "best_model = None\n",
    "best_params = {}\n",
    "best_score = 0.0\n",
    "budget = 600. # time budget in seconds\n",
    "\n",
    "# Random search\n",
    "while budget > 0:\n",
    "\n",
    "    # Step 1 - Sample a model uniformally\n",
    "    [key] = random.sample(list(models), 1)\n",
    "    model = models[key]\n",
    "    hp_space = search_space[key]\n",
    "    # Step 2 - Sample a configuration from its hyperparam space\n",
    "    params = hp_space.sample_configuration().get_dictionary()\n",
    "    model.set_params(**params)\n",
    "\n",
    "    # cv strategy\n",
    "    cv = StratifiedShuffleSplit(n_splits=5, test_size=0.25)\n",
    "    # Evaluate\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    budget -= elapsed\n",
    "    evaluated[key] += 1\n",
    "\n",
    "    # Update best model\n",
    "    if scores.mean() > best_score:\n",
    "        best_model = key\n",
    "        best_score = scores.mean()\n",
    "        best_params = params\n",
    "\n",
    "# General results\n",
    "for key, val in evaluated.items():\n",
    "    print(key + ':\\t', val)\n",
    "print('Best model:', best_model)\n",
    "print('Hyperparameter configuration:', best_params)\n",
    "print('ROC AUC score:', best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance:\n",
      "\tROC AUC score:\t\t 0.9318530612244899\n",
      "\tAverage pecision score:\t 0.37949347418215923\n"
     ]
    }
   ],
   "source": [
    "# Apply best model on test set\n",
    "final_model = models[best_model].set_params(**best_params)\n",
    "final_model.fit(X_train, y_train) # refit\n",
    "y_pred = final_model.predict_proba(X_test)\n",
    "auc_score = roc_auc_score(y_test, y_pred[:,1])\n",
    "avg_prec_score = average_precision_score(y_test, y_pred[:,1])\n",
    "print('Test performance:')\n",
    "print('\\tROC AUC score:\\t\\t', auc_score)\n",
    "print('\\tAverage pecision score:\\t', avg_prec_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
